{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giressesama237/Giresse-Bilal/blob/main/IoT_Based_Air_Quality_Monitoring_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IoT-Based Air Quality Monitoring System\n",
        "## Machine Learning Model Comparison: LSTM vs Random Forest vs Linear Regression\n",
        "\n",
        "**Dataset:** UCI Air Quality Dataset (9,358 hourly measurements)  \n",
        "**Period:** March 2004 - February 2005  \n",
        "**Location:** Polluted urban area, Italy  \n",
        "**Objective:** Compare three ML models for air pollution forecasting\n",
        "\n",
        "**Authors:** Giresse & Geethmal\n",
        "**Date:** December 2024"
      ],
      "metadata": {
        "id": "TCIpYwkeYl_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Install and Import Required Libraries\n",
        "\n",
        "# Install additional packages if needed\n",
        "!pip install -q ucimlrepo\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# TensorFlow/Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swukoKkQY0Xl",
        "outputId": "6fc0e750-8c8f-4084-a3d4-8050883125f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "TensorFlow version: 2.19.0\n",
            "NumPy version: 2.0.2\n",
            "Pandas version: 2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load and Explore UCI Air Quality Dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Fetch dataset\n",
        "air_quality = fetch_ucirepo(id=360)\n",
        "\n",
        "# Get features and combine into single dataframe\n",
        "X = air_quality.data.features\n",
        "df = X.copy()\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMissing values:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCXb37QIaBMW",
        "outputId": "e0be5b4a-e3f7-456a-d504-e69b3ca65967"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "\n",
            "Dataset shape: (9357, 15)\n",
            "\n",
            "First few rows:\n",
            "        Date      Time  CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  \\\n",
            "0  3/10/2004  18:00:00     2.6         1360       150      11.9   \n",
            "1  3/10/2004  19:00:00     2.0         1292       112       9.4   \n",
            "2  3/10/2004  20:00:00     2.2         1402        88       9.0   \n",
            "3  3/10/2004  21:00:00     2.2         1376        80       9.2   \n",
            "4  3/10/2004  22:00:00     1.6         1272        51       6.5   \n",
            "\n",
            "   PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)  \\\n",
            "0           1046      166          1056      113          1692         1268   \n",
            "1            955      103          1174       92          1559          972   \n",
            "2            939      131          1140      114          1555         1074   \n",
            "3            948      172          1092      122          1584         1203   \n",
            "4            836      131          1205      116          1490         1110   \n",
            "\n",
            "      T    RH      AH  \n",
            "0  13.6  48.9  0.7578  \n",
            "1  13.3  47.7  0.7255  \n",
            "2  11.9  54.0  0.7502  \n",
            "3  11.0  60.0  0.7867  \n",
            "4  11.2  59.6  0.7888  \n",
            "\n",
            "Column names:\n",
            "['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
            "\n",
            "Data types:\n",
            "Date              object\n",
            "Time              object\n",
            "CO(GT)           float64\n",
            "PT08.S1(CO)        int64\n",
            "NMHC(GT)           int64\n",
            "C6H6(GT)         float64\n",
            "PT08.S2(NMHC)      int64\n",
            "NOx(GT)            int64\n",
            "PT08.S3(NOx)       int64\n",
            "NO2(GT)            int64\n",
            "PT08.S4(NO2)       int64\n",
            "PT08.S5(O3)        int64\n",
            "T                float64\n",
            "RH               float64\n",
            "AH               float64\n",
            "dtype: object\n",
            "\n",
            "Missing values:\n",
            "Date             0\n",
            "Time             0\n",
            "CO(GT)           0\n",
            "PT08.S1(CO)      0\n",
            "NMHC(GT)         0\n",
            "C6H6(GT)         0\n",
            "PT08.S2(NMHC)    0\n",
            "NOx(GT)          0\n",
            "PT08.S3(NOx)     0\n",
            "NO2(GT)          0\n",
            "PT08.S4(NO2)     0\n",
            "PT08.S5(O3)      0\n",
            "T                0\n",
            "RH               0\n",
            "AH               0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Data Cleaning with Smart Missing Value Handling\n",
        "\n",
        "# Remove empty columns (if any)\n",
        "df = df.dropna(how='all', axis=1)\n",
        "\n",
        "print(\"Initial dataset information:\")\n",
        "print(f\"   Total rows: {len(df):,}\")\n",
        "print(f\"   Total columns: {len(df.columns)}\")\n",
        "\n",
        "# Handle missing values (-200.0 indicates missing data)\n",
        "print(\"\\n Checking for missing values marked as -200.0...\")\n",
        "\n",
        "# List of numeric columns to check\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Replace -200.0 with NaN\n",
        "for col in numeric_cols:\n",
        "    df[col] = df[col].replace(-200.0, np.nan)\n",
        "\n",
        "# Display missing value statistics BEFORE cleaning\n",
        "print(f\"\\n Missing values per column:\")\n",
        "missing_stats = df.isnull().sum().sort_values(ascending=False)\n",
        "missing_stats_pct = (missing_stats / len(df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_stats[missing_stats > 0],\n",
        "    'Percentage': missing_stats_pct[missing_stats > 0]\n",
        "})\n",
        "print(missing_df)\n",
        "\n",
        "# STRATEGY 1: Keep rows where TARGET variable is NOT missing\n",
        "target_column = 'CO(GT)'\n",
        "\n",
        "print(f\"\\n Strategy: Keep rows where target '{target_column}' is NOT missing\")\n",
        "\n",
        "# First, remove rows where target is missing\n",
        "df_clean = df[df[target_column].notna()].copy()\n",
        "\n",
        "print(f\"\\n After removing rows with missing target:\")\n",
        "print(f\"   Remaining rows: {len(df_clean):,} ({len(df_clean)/len(df)*100:.1f}% retained)\")\n",
        "\n",
        "# STRATEGY 2: Impute missing values for FEATURES (not target)\n",
        "# Define feature columns\n",
        "feature_columns = [\n",
        "    'PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)',\n",
        "    'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH'\n",
        "]\n",
        "\n",
        "# Check which features exist\n",
        "available_features = [col for col in feature_columns if col in df_clean.columns]\n",
        "\n",
        "print(f\"\\nğŸ”§ Imputing missing values for features using forward-fill + interpolation...\")\n",
        "\n",
        "# Forward fill (use previous value) then interpolate remaining gaps\n",
        "for col in available_features:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        missing_before = df_clean[col].isnull().sum()\n",
        "\n",
        "        # Method 1: Forward fill (use last valid observation)\n",
        "        df_clean[col] = df_clean[col].fillna(method='ffill')\n",
        "\n",
        "        # Method 2: Backward fill for any remaining NaNs at start\n",
        "        df_clean[col] = df_clean[col].fillna(method='bfill')\n",
        "\n",
        "        # Method 3: Linear interpolation for any remaining gaps\n",
        "        df_clean[col] = df_clean[col].interpolate(method='linear')\n",
        "\n",
        "        missing_after = df_clean[col].isnull().sum()\n",
        "        print(f\"   {col}: {missing_before} â†’ {missing_after} missing values\")\n",
        "\n",
        "# Final check: remove any rows that still have missing values (should be very few)\n",
        "rows_before = len(df_clean)\n",
        "df_clean = df_clean.dropna(subset=available_features + [target_column])\n",
        "rows_after = len(df_clean)\n",
        "\n",
        "if rows_before != rows_after:\n",
        "    print(f\"\\n Removed {rows_before - rows_after} rows with remaining missing values\")\n",
        "\n",
        "print(f\"\\n Final cleaned dataset:\")\n",
        "print(f\"   Total rows: {len(df_clean):,}\")\n",
        "print(f\"   Retention rate: {len(df_clean)/len(df)*100:.1f}%\")\n",
        "print(f\"   Rows lost: {len(df) - len(df_clean):,}\")\n",
        "\n",
        "# Create datetime column if Date and Time exist\n",
        "if 'Date' in df_clean.columns and 'Time' in df_clean.columns:\n",
        "    df_clean['DateTime'] = pd.to_datetime(df_clean['Date'].astype(str) + ' ' +\n",
        "                                          df_clean['Time'].astype(str),\n",
        "                                          errors='coerce')\n",
        "    df_clean = df_clean.sort_values('DateTime')\n",
        "    df_clean = df_clean.reset_index(drop=True)\n",
        "    print(f\"\\n DateTime column created and sorted\")\n",
        "\n",
        "# Display cleaned dataset info\n",
        "print(f\"\\nDate range: {df_clean['DateTime'].min()} to {df_clean['DateTime'].max()}\")\n",
        "print(f\"\\n Final missing values check:\")\n",
        "print(df_clean[available_features + [target_column]].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5AJpIdPaaGB",
        "outputId": "88bb72cc-f85a-414e-bb09-ee8674f6c925"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial dataset information:\n",
            "   Total rows: 9,357\n",
            "   Total columns: 15\n",
            "\n",
            " Checking for missing values marked as -200.0...\n",
            "\n",
            " Missing values per column:\n",
            "               Missing Count  Percentage\n",
            "NMHC(GT)                8443       90.23\n",
            "CO(GT)                  1683       17.99\n",
            "NO2(GT)                 1642       17.55\n",
            "NOx(GT)                 1639       17.52\n",
            "PT08.S2(NMHC)            366        3.91\n",
            "C6H6(GT)                 366        3.91\n",
            "PT08.S1(CO)              366        3.91\n",
            "PT08.S5(O3)              366        3.91\n",
            "T                        366        3.91\n",
            "PT08.S3(NOx)             366        3.91\n",
            "PT08.S4(NO2)             366        3.91\n",
            "RH                       366        3.91\n",
            "AH                       366        3.91\n",
            "\n",
            " Strategy: Keep rows where target 'CO(GT)' is NOT missing\n",
            "\n",
            " After removing rows with missing target:\n",
            "   Remaining rows: 7,674 (82.0% retained)\n",
            "\n",
            "ğŸ”§ Imputing missing values for features using forward-fill + interpolation...\n",
            "   PT08.S1(CO): 330 â†’ 0 missing values\n",
            "   PT08.S2(NMHC): 330 â†’ 0 missing values\n",
            "   PT08.S3(NOx): 330 â†’ 0 missing values\n",
            "   PT08.S4(NO2): 330 â†’ 0 missing values\n",
            "   PT08.S5(O3): 330 â†’ 0 missing values\n",
            "   T: 330 â†’ 0 missing values\n",
            "   RH: 330 â†’ 0 missing values\n",
            "   AH: 330 â†’ 0 missing values\n",
            "\n",
            " Final cleaned dataset:\n",
            "   Total rows: 7,674\n",
            "   Retention rate: 82.0%\n",
            "   Rows lost: 1,683\n",
            "\n",
            " DateTime column created and sorted\n",
            "\n",
            "Date range: 2004-03-10 18:00:00 to 2005-04-04 14:00:00\n",
            "\n",
            " Final missing values check:\n",
            "PT08.S1(CO)      0\n",
            "PT08.S2(NMHC)    0\n",
            "PT08.S3(NOx)     0\n",
            "PT08.S4(NO2)     0\n",
            "PT08.S5(O3)      0\n",
            "T                0\n",
            "RH               0\n",
            "AH               0\n",
            "CO(GT)           0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Feature Selection for Model Training\n",
        "\n",
        "# Select relevant features for prediction\n",
        "# We'll predict CO(GT) using sensor readings and environmental factors\n",
        "\n",
        "feature_columns = [\n",
        "    'PT08.S1(CO)',      # CO sensor\n",
        "    'PT08.S2(NMHC)',    # NMHC sensor\n",
        "    'PT08.S3(NOx)',     # NOx sensor\n",
        "    'PT08.S4(NO2)',     # NO2 sensor\n",
        "    'PT08.S5(O3)',      # O3 sensor\n",
        "    'T',                # Temperature\n",
        "    'RH',               # Relative Humidity\n",
        "    'AH'                # Absolute Humidity\n",
        "]\n",
        "\n",
        "target_column = 'CO(GT)'  # Target: CO concentration\n",
        "\n",
        "# Check if all columns exist\n",
        "available_features = [col for col in feature_columns if col in df_clean.columns]\n",
        "print(f\" Available features: {len(available_features)}/{len(feature_columns)}\")\n",
        "print(f\"Features: {available_features}\")\n",
        "\n",
        "# Create feature matrix and target vector\n",
        "X_data = df_clean[available_features].values\n",
        "y_data = df_clean[target_column].values\n",
        "\n",
        "print(f\"\\n Feature matrix shape: {X_data.shape}\")\n",
        "print(f\" Target vector shape: {y_data.shape}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(f\"\\n Target variable (CO) statistics:\")\n",
        "print(f\"Mean: {y_data.mean():.2f} mg/mÂ³\")\n",
        "print(f\"Std: {y_data.std():.2f} mg/mÂ³\")\n",
        "print(f\"Min: {y_data.min():.2f} mg/mÂ³\")\n",
        "print(f\"Max: {y_data.max():.2f} mg/mÂ³\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u40cfdWQbrt1",
        "outputId": "63fc7d86-64df-4560-e981-bdbf67c57d7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Available features: 8/8\n",
            "Features: ['PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
            "\n",
            " Feature matrix shape: (7674, 8)\n",
            " Target vector shape: (7674,)\n",
            "\n",
            " Target variable (CO) statistics:\n",
            "Mean: 2.15 mg/mÂ³\n",
            "Std: 1.45 mg/mÂ³\n",
            "Min: 0.10 mg/mÂ³\n",
            "Max: 11.90 mg/mÂ³\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Temporal Train/Validation/Test Split\n",
        "# IMPORTANT: No random shuffling for time-series data!\n",
        "\n",
        "# Split ratios\n",
        "train_ratio = 0.70\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Calculate split indices\n",
        "n_samples = len(X_data)\n",
        "train_end = int(n_samples * train_ratio)\n",
        "val_end = int(n_samples * (train_ratio + val_ratio))\n",
        "\n",
        "# Perform temporal split\n",
        "X_train = X_data[:train_end]\n",
        "y_train = y_data[:train_end]\n",
        "\n",
        "X_val = X_data[train_end:val_end]\n",
        "y_val = y_data[train_end:val_end]\n",
        "\n",
        "X_test = X_data[val_end:]\n",
        "y_test = y_data[val_end:]\n",
        "\n",
        "print(\" Temporal split completed!\")\n",
        "print(f\"\\n Training set: {len(X_train)} samples ({train_ratio*100:.0f}%)\")\n",
        "print(f\" Validation set: {len(X_val)} samples ({val_ratio*100:.0f}%)\")\n",
        "print(f\" Test set: {len(X_test)} samples ({test_ratio*100:.0f}%)\")\n",
        "\n",
        "# Display date ranges if DateTime exists\n",
        "if 'DateTime' in df_clean.columns:\n",
        "    print(f\"\\n Training period: {df_clean['DateTime'].iloc[0]} to {df_clean['DateTime'].iloc[train_end-1]}\")\n",
        "    print(f\" Validation period: {df_clean['DateTime'].iloc[train_end]} to {df_clean['DateTime'].iloc[val_end-1]}\")\n",
        "    print(f\" Test period: {df_clean['DateTime'].iloc[val_end]} to {df_clean['DateTime'].iloc[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHt2LhtXb7Xn",
        "outputId": "0713284e-505c-4724-a11a-f2cb9b5dceb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Temporal split completed!\n",
            "\n",
            " Training set: 5371 samples (70%)\n",
            " Validation set: 1151 samples (15%)\n",
            " Test set: 1152 samples (15%)\n",
            "\n",
            " Training period: 2004-03-10 18:00:00 to 2004-12-21 11:00:00\n",
            " Validation period: 2004-12-21 12:00:00 to 2005-02-14 11:00:00\n",
            " Test period: 2005-02-14 12:00:00 to 2005-04-04 14:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Data Normalization (Min-Max Scaling)\n",
        "# CRITICAL: Fit scaler only on training data to prevent data leakage!\n",
        "\n",
        "# Initialize scaler\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "# Fit on training data only\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Transform validation and test data using training parameters\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(\" Data normalization completed!\")\n",
        "print(f\"\\n Scaled feature range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n",
        "print(f\" Scaled target range: [{y_train_scaled.min():.3f}, {y_train_scaled.max():.3f}]\")\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"\\n Training shapes:\")\n",
        "print(f\"   X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"   y_train_scaled: {y_train_scaled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AoxGoJNcMSo",
        "outputId": "7f07ec68-9c2f-4a18-cc78-29ad120b0e70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Data normalization completed!\n",
            "\n",
            " Scaled feature range: [0.000, 1.000]\n",
            " Scaled target range: [0.000, 1.000]\n",
            "\n",
            " Training shapes:\n",
            "   X_train_scaled: (5371, 8)\n",
            "   y_train_scaled: (5371,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create Temporal Sequences for LSTM Model\n",
        "\n",
        "def create_sequences(X, y, seq_length=24):\n",
        "    \"\"\"\n",
        "    Create temporal sequences for LSTM training\n",
        "\n",
        "    Args:\n",
        "        X: Feature array (samples, features)\n",
        "        y: Target array (samples,)\n",
        "        seq_length: Number of time steps to look back (default: 24 hours)\n",
        "\n",
        "    Returns:\n",
        "        X_seq: Sequences (samples, timesteps, features)\n",
        "        y_seq: Targets (samples,)\n",
        "    \"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "\n",
        "    for i in range(len(X) - seq_length):\n",
        "        # Input: previous seq_length time steps\n",
        "        X_seq.append(X[i:i+seq_length])\n",
        "        # Output: value at current time step\n",
        "        y_seq.append(y[i+seq_length])\n",
        "\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create sequences with 24-hour lookback window\n",
        "SEQUENCE_LENGTH = 24\n",
        "\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, SEQUENCE_LENGTH)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, SEQUENCE_LENGTH)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "print(\" Sequences created for LSTM!\")\n",
        "print(f\"\\n Sequence shapes:\")\n",
        "print(f\"   X_train_seq: {X_train_seq.shape} (samples, timesteps, features)\")\n",
        "print(f\"   y_train_seq: {y_train_seq.shape}\")\n",
        "print(f\"   X_val_seq: {X_val_seq.shape}\")\n",
        "print(f\"   y_val_seq: {y_val_seq.shape}\")\n",
        "print(f\"   X_test_seq: {X_test_seq.shape}\")\n",
        "print(f\"   y_test_seq: {y_test_seq.shape}\")\n",
        "\n",
        "print(f\"\\n Each sample uses {SEQUENCE_LENGTH} previous hours to predict the next hour\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK2goX20cYXX",
        "outputId": "d8113ac9-b66f-4ea1-8683-e5be88fa5715"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sequences created for LSTM!\n",
            "\n",
            " Sequence shapes:\n",
            "   X_train_seq: (5347, 24, 8) (samples, timesteps, features)\n",
            "   y_train_seq: (5347,)\n",
            "   X_val_seq: (1127, 24, 8)\n",
            "   y_val_seq: (1127,)\n",
            "   X_test_seq: (1128, 24, 8)\n",
            "   y_test_seq: (1128,)\n",
            "\n",
            " Each sample uses 24 previous hours to predict the next hour\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Create Temporal Sequences for LSTM Model\n",
        "\n",
        "def create_sequences(X, y, seq_length=24):\n",
        "    \"\"\"\n",
        "    Create temporal sequences for LSTM training\n",
        "\n",
        "    Args:\n",
        "        X: Feature array (samples, features)\n",
        "        y: Target array (samples,)\n",
        "        seq_length: Number of time steps to look back (default: 24 hours)\n",
        "\n",
        "    Returns:\n",
        "        X_seq: Sequences (samples, timesteps, features)\n",
        "        y_seq: Targets (samples,)\n",
        "    \"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "\n",
        "    for i in range(len(X) - seq_length):\n",
        "        # Input: previous seq_length time steps\n",
        "        X_seq.append(X[i:i+seq_length])\n",
        "        # Output: value at current time step\n",
        "        y_seq.append(y[i+seq_length])\n",
        "\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create sequences with 24-hour lookback window\n",
        "SEQUENCE_LENGTH = 24\n",
        "\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, SEQUENCE_LENGTH)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, SEQUENCE_LENGTH)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "print(\" Sequences created for LSTM!\")\n",
        "print(f\"\\n Sequence shapes:\")\n",
        "print(f\"   X_train_seq: {X_train_seq.shape} (samples, timesteps, features)\")\n",
        "print(f\"   y_train_seq: {y_train_seq.shape}\")\n",
        "print(f\"   X_val_seq: {X_val_seq.shape}\")\n",
        "print(f\"   y_val_seq: {y_val_seq.shape}\")\n",
        "print(f\"   X_test_seq: {X_test_seq.shape}\")\n",
        "print(f\"   y_test_seq: {y_test_seq.shape}\")\n",
        "\n",
        "print(f\"\\n Each sample uses {SEQUENCE_LENGTH} previous hours to predict the next hour\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32nNYF_Vc8SP",
        "outputId": "e4ee02eb-2fd4-47ae-907c-16a460857ed5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sequences created for LSTM!\n",
            "\n",
            " Sequence shapes:\n",
            "   X_train_seq: (5347, 24, 8) (samples, timesteps, features)\n",
            "   y_train_seq: (5347,)\n",
            "   X_val_seq: (1127, 24, 8)\n",
            "   y_val_seq: (1127,)\n",
            "   X_test_seq: (1128, 24, 8)\n",
            "   y_test_seq: (1128,)\n",
            "\n",
            " Each sample uses 24 previous hours to predict the next hour\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Build LSTM Neural Network Model\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    \"\"\"\n",
        "    Build 3-layer LSTM architecture\n",
        "\n",
        "    Args:\n",
        "        input_shape: Tuple (timesteps, features)\n",
        "\n",
        "    Returns:\n",
        "        Compiled LSTM model\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # First LSTM layer: 128 units\n",
        "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Second LSTM layer: 64 units\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Third LSTM layer: 32 units\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Dense layers\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)  # Output layer\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "lstm_model = build_lstm_model(input_shape=(SEQUENCE_LENGTH, X_train_scaled.shape[1]))\n",
        "\n",
        "# Display model architecture\n",
        "print(\"LSTM Model Architecture:\")\n",
        "lstm_model.summary()\n",
        "\n",
        "# Count parameters\n",
        "total_params = lstm_model.count_params()\n",
        "print(f\"\\n Total parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "TLxpqmSTdEYs",
        "outputId": "f119f849-b293-4d30-a00e-a9ad45253ee0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m70,144\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m49,408\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m17\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,144</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m132,513\u001b[0m (517.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,513</span> (517.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m132,513\u001b[0m (517.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,513</span> (517.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Total parameters: 132,513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Train LSTM Model\n",
        "\n",
        "# Define callbacks\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"ğŸš€ Training LSTM model...\")\n",
        "print(\"This may take 10-20 minutes depending on your hardware\\n\")\n",
        "\n",
        "history = lstm_model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nLSTM training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpchXXTUdT2U",
        "outputId": "c83bfc6e-a853-4729-bb6d-484b3269a7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Training LSTM model...\n",
            "This may take 10-20 minutes depending on your hardware\n",
            "\n",
            "Epoch 1/50\n",
            "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 112ms/step - loss: 0.0160 - mae: 0.0965 - val_loss: 0.0157 - val_mae: 0.0971 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - loss: 0.0120 - mae: 0.0806 - val_loss: 0.0135 - val_mae: 0.0885 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 59ms/step - loss: 0.0105 - mae: 0.0760 - val_loss: 0.0136 - val_mae: 0.0896 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 59ms/step - loss: 0.0091 - mae: 0.0702 - val_loss: 0.0130 - val_mae: 0.0871 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.0086 - mae: 0.0682 - val_loss: 0.0119 - val_mae: 0.0818 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 51ms/step - loss: 0.0079 - mae: 0.0648 - val_loss: 0.0105 - val_mae: 0.0770 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - loss: 0.0070 - mae: 0.0613 - val_loss: 0.0111 - val_mae: 0.0798 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m 84/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - loss: 0.0059 - mae: 0.0552"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Visualize LSTM Training History\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot loss\n",
        "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_title('LSTM Model Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Mean Squared Error')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot MAE\n",
        "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "axes[1].set_title('LSTM Model MAE', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Mean Absolute Error')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best epoch: {np.argmin(history.history['val_loss']) + 1}\")\n",
        "print(f\"Best validation loss: {min(history.history['val_loss']):.6f}\")"
      ],
      "metadata": {
        "id": "6OjEb3CHd3C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Build and Train Random Forest Model\n",
        "\n",
        "print(\"Training Random Forest model...\")\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,        # Number of trees\n",
        "    max_depth=10,            # Maximum tree depth\n",
        "    min_samples_split=5,     # Minimum samples to split\n",
        "    min_samples_leaf=2,      # Minimum samples in leaf\n",
        "    bootstrap=True,          # Bootstrap sampling\n",
        "    random_state=42,\n",
        "    n_jobs=-1,               # Use all CPU cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train model (no sequences needed - uses tabular data)\n",
        "rf_model.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "print(\"\\n Random Forest training completed!\")\n",
        "\n",
        "# Display feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': available_features,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n Feature Importance Ranking:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Random Forest Feature Importance', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ktA8lNTjs_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Build and Train Ridge Linear Regression with Polynomial Features\n",
        "\n",
        "print(\"Training Linear Regression model...\")\n",
        "\n",
        "# Create polynomial features (degree 2 for non-linearity)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# Transform features\n",
        "X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "X_val_poly = poly.transform(X_val_scaled)\n",
        "X_test_poly = poly.transform(X_test_scaled)\n",
        "\n",
        "print(f\"Polynomial features created: {X_train_poly.shape[1]} features\")\n",
        "\n",
        "# Train Ridge Regression (with regularization)\n",
        "lr_model = Ridge(alpha=1.0, random_state=42)\n",
        "lr_model.fit(X_train_poly, y_train_scaled)\n",
        "\n",
        "print(\"Linear Regression training completed!\")\n",
        "\n",
        "# Display model coefficients\n",
        "print(f\"\\n Model statistics:\")\n",
        "print(f\"   Number of coefficients: {len(lr_model.coef_)}\")\n",
        "print(f\"   Intercept: {lr_model.intercept_:.6f}\")\n",
        "print(f\"   Regularization (alpha): {lr_model.alpha}\")"
      ],
      "metadata": {
        "id": "2Kwcfi4ZkqCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Generate Predictions on Test Set (ALIGNED)\n",
        "\n",
        "print(\" Generating predictions on test set...\\n\")\n",
        "\n",
        "# IMPORTANT: All models must predict on the SAME samples for fair comparison\n",
        "# LSTM uses sequences, so we align RF and LR to match\n",
        "\n",
        "# Step 1: LSTM predictions (uses sequences)\n",
        "print(\"LSTM predictions...\")\n",
        "lstm_pred_scaled = lstm_model.predict(X_test_seq, verbose=0)\n",
        "# Inverse transform to original scale\n",
        "lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled).flatten()\n",
        "# Corresponding true values\n",
        "y_test_lstm = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"   LSTM predictions shape: {lstm_pred.shape}\")\n",
        "print(f\"   LSTM can predict on {len(lstm_pred)} samples (due to sequence requirement)\")\n",
        "\n",
        "# Step 2: Align Random Forest and Linear Regression\n",
        "# Skip the first SEQUENCE_LENGTH samples to align with LSTM\n",
        "print(f\"\\n Aligning Random Forest and Linear Regression...\")\n",
        "print(f\"   Skipping first {SEQUENCE_LENGTH} samples to match LSTM sequences\")\n",
        "\n",
        "# RF predictions - aligned with LSTM\n",
        "# We need to skip first SEQUENCE_LENGTH samples from test set\n",
        "X_test_scaled_aligned = X_test_scaled[SEQUENCE_LENGTH:SEQUENCE_LENGTH + len(lstm_pred)]\n",
        "y_test_aligned = y_test[SEQUENCE_LENGTH:SEQUENCE_LENGTH + len(lstm_pred)]\n",
        "\n",
        "# Random Forest predictions\n",
        "rf_pred_scaled = rf_model.predict(X_test_scaled_aligned)\n",
        "rf_pred = scaler_y.inverse_transform(rf_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"   Random Forest predictions shape: {rf_pred.shape}\")\n",
        "\n",
        "# Step 3: Linear Regression predictions (polynomial features)\n",
        "X_test_poly_aligned = X_test_poly[SEQUENCE_LENGTH:SEQUENCE_LENGTH + len(lstm_pred)]\n",
        "lr_pred_scaled = lr_model.predict(X_test_poly_aligned)\n",
        "lr_pred = scaler_y.inverse_transform(lr_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"   Linear Regression predictions shape: {lr_pred.shape}\")\n",
        "\n",
        "# Verify all predictions have same length\n",
        "print(f\"\\n Alignment verification:\")\n",
        "print(f\"   True values: {len(y_test_aligned)} samples\")\n",
        "print(f\"   LSTM predictions: {len(lstm_pred)} samples\")\n",
        "print(f\"   RF predictions: {len(rf_pred)} samples\")\n",
        "print(f\"   LR predictions: {len(lr_pred)} samples\")\n",
        "\n",
        "# Check if all have same length\n",
        "if len(y_test_aligned) == len(lstm_pred) == len(rf_pred) == len(lr_pred):\n",
        "    print(f\"\\n All predictions ALIGNED correctly!\")\n",
        "else:\n",
        "    print(f\"\\n WARNING: Predictions have different lengths!\")\n",
        "    print(f\"   Truncating to minimum length...\")\n",
        "    min_len = min(len(y_test_aligned), len(lstm_pred), len(rf_pred), len(lr_pred))\n",
        "    y_test_aligned = y_test_aligned[:min_len]\n",
        "    lstm_pred = lstm_pred[:min_len]\n",
        "    rf_pred = rf_pred[:min_len]\n",
        "    lr_pred = lr_pred[:min_len]\n",
        "    print(f\"   Final length: {min_len} samples\")"
      ],
      "metadata": {
        "id": "zFDIwrG7kzSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15.5: DEBUGGING LSTM Predictions\n",
        "\n",
        "print(\"DEBUGGING LSTM Predictions...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Check shapes\n",
        "print(\"\\n Shapes Check:\")\n",
        "print(f\"   lstm_pred_scaled shape: {lstm_pred_scaled.shape}\")\n",
        "print(f\"   lstm_pred (after inverse) shape: {lstm_pred.shape}\")\n",
        "print(f\"   y_test_aligned shape: {y_test_aligned.shape}\")\n",
        "\n",
        "# 2. Check value ranges\n",
        "print(\"\\n Value Ranges Check:\")\n",
        "print(f\"   lstm_pred_scaled range: [{lstm_pred_scaled.min():.4f}, {lstm_pred_scaled.max():.4f}]\")\n",
        "print(f\"   lstm_pred range: [{lstm_pred.min():.4f}, {lstm_pred.max():.4f}]\")\n",
        "print(f\"   y_test_aligned range: [{y_test_aligned.min():.4f}, {y_test_aligned.max():.4f}]\")\n",
        "\n",
        "# 3. Check first 10 predictions vs true values\n",
        "print(\"\\n First 10 Predictions vs True Values:\")\n",
        "print(f\"{'Index':<10} {'True Value':<15} {'LSTM Pred':<15} {'Error':<15}\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(min(10, len(y_test_aligned))):\n",
        "    error = abs(y_test_aligned[i] - lstm_pred[i])\n",
        "    print(f\"{i:<10} {y_test_aligned[i]:<15.4f} {lstm_pred[i]:<15.4f} {error:<15.4f}\")\n",
        "\n",
        "# 4. Check if predictions are constant (model collapsed)\n",
        "lstm_std = np.std(lstm_pred)\n",
        "print(f\"\\n Prediction Variability:\")\n",
        "print(f\"   LSTM predictions std dev: {lstm_std:.6f}\")\n",
        "if lstm_std < 0.01:\n",
        "    print(\"    WARNING: LSTM predictions are nearly constant!\")\n",
        "else:\n",
        "    print(\"    LSTM predictions vary normally\")\n",
        "\n",
        "# 5. Visualize first 100 predictions\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 5))\n",
        "n_vis = min(100, len(y_test_aligned))\n",
        "plt.plot(range(n_vis), y_test_aligned[:n_vis], label='True Values', linewidth=2, color='black')\n",
        "plt.plot(range(n_vis), lstm_pred[:n_vis], label='LSTM Predictions', linewidth=1.5, color='blue', alpha=0.7)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('CO Concentration')\n",
        "plt.title('LSTM Predictions vs True Values (First 100 Samples)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 6. Check for NaN or Inf\n",
        "print(f\"\\n Data Quality Check:\")\n",
        "print(f\"   LSTM predictions - NaN count: {np.isnan(lstm_pred).sum()}\")\n",
        "print(f\"   LSTM predictions - Inf count: {np.isinf(lstm_pred).sum()}\")\n",
        "print(f\"   True values - NaN count: {np.isnan(y_test_aligned).sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ],
      "metadata": {
        "id": "bpIt3ZW1soa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Calculate Performance Metrics for All Models (ALIGNED)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Calculate regression metrics\n",
        "\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "        model_name: Name of the model\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
        "    # Avoid division by zero\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'RÂ²': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MAPE (%)': mape\n",
        "    }\n",
        "\n",
        "# Calculate metrics for each model using ALIGNED predictions\n",
        "print(\" Calculating performance metrics on ALIGNED test set...\\n\")\n",
        "\n",
        "# All models use the SAME y_test_aligned for fair comparison\n",
        "lstm_metrics = calculate_metrics(y_test_aligned, lstm_pred, 'LSTM')\n",
        "rf_metrics = calculate_metrics(y_test_aligned, rf_pred, 'Random Forest')\n",
        "lr_metrics = calculate_metrics(y_test_aligned, lr_pred, 'Linear Regression')\n",
        "\n",
        "# Create comparison dataframe\n",
        "results_df = pd.DataFrame([lstm_metrics, rf_metrics, lr_metrics])\n",
        "\n",
        "print(\" Performance Metrics on Test Set (ALIGNED):\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verify RÂ² values are reasonable\n",
        "print(\"\\n Sanity check:\")\n",
        "for _, row in results_df.iterrows():\n",
        "    if row['RÂ²'] < 0 or row['RÂ²'] > 1:\n",
        "        print(f\"    {row['Model']}: RÂ² = {row['RÂ²']:.4f} (UNUSUAL!)\")\n",
        "    else:\n",
        "        print(f\"    {row['Model']}: RÂ² = {row['RÂ²']:.4f} (OK)\")\n",
        "\n",
        "# Highlight best model for each metric\n",
        "print(\"\\n Best performing model per metric:\")\n",
        "print(f\"   Highest RÂ²: {results_df.loc[results_df['RÂ²'].idxmax(), 'Model']} ({results_df['RÂ²'].max():.4f})\")\n",
        "print(f\"   Lowest RMSE: {results_df.loc[results_df['RMSE'].idxmin(), 'Model']} ({results_df['RMSE'].min():.4f})\")\n",
        "print(f\"   Lowest MAE: {results_df.loc[results_df['MAE'].idxmin(), 'Model']} ({results_df['MAE'].min():.4f})\")\n",
        "print(f\"   Lowest MAPE: {results_df.loc[results_df['MAPE (%)'].idxmin(), 'Model']} ({results_df['MAPE (%)'].min():.2f}%)\")"
      ],
      "metadata": {
        "id": "_DQoAu73lBAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Weighted Ensemble (Optimized Weights)\n",
        "\n",
        "print(\" Creating weighted ensemble model...\\n\")\n",
        "\n",
        "# Poids basÃ©s sur performance:\n",
        "# RF (meilleur) = 50%\n",
        "# LR (2Ã¨me) = 30%\n",
        "# LSTM (3Ã¨me) = 20%\n",
        "\n",
        "ensemble_pred = (0.5 * rf_pred +\n",
        "                 0.3 * lr_pred +\n",
        "                 0.2 * lstm_pred)\n",
        "\n",
        "print(f\" Ensemble predictions shape: {ensemble_pred.shape}\")\n",
        "\n",
        "# Calculate ensemble metrics\n",
        "ensemble_metrics = calculate_metrics(y_test_aligned, ensemble_pred, 'Ensemble (0.5RF+0.3LR+0.2LSTM)')\n",
        "\n",
        "# Add to results\n",
        "results_df = pd.concat([results_df, pd.DataFrame([ensemble_metrics])], ignore_index=True)\n",
        "\n",
        "print(\"\\n Final Model Comparison (including Ensemble):\")\n",
        "print(\"=\" * 85)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 85)\n",
        "\n",
        "# Compare ensemble vs best individual\n",
        "best_individual_r2 = results_df[results_df['Model'] != 'Ensemble (0.5RF+0.3LR+0.2LSTM)']['RÂ²'].max()\n",
        "ensemble_r2 = ensemble_metrics['RÂ²']\n",
        "improvement = ((ensemble_r2 - best_individual_r2) / best_individual_r2) * 100\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"\\n Ensemble improvement: +{improvement:.2f}%\")\n",
        "    print(f\" Ensemble RÂ² = {ensemble_r2:.4f} vs Best Individual RÂ² = {best_individual_r2:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n Ensemble RÂ² = {ensemble_r2:.4f} (comparable to best individual)\")"
      ],
      "metadata": {
        "id": "_ONPHlXymwzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Visualize Model Predictions (SEPARATE FIGURES)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_timeseries_single(y_true, y_pred, model_name, r2, color='blue', n_samples=500, filename=None):\n",
        "    \"\"\"\n",
        "    Create individual time-series prediction plot for one model\n",
        "\n",
        "    Args:\n",
        "        y_true: True CO values\n",
        "        y_pred: Predicted CO values\n",
        "        model_name: Name of the model\n",
        "        r2: RÂ² score\n",
        "        color: Color for predictions line\n",
        "        n_samples: Number of samples to plot\n",
        "        filename: Path to save figure\n",
        "    \"\"\"\n",
        "    n_samples = min(n_samples, len(y_true))\n",
        "    x_axis = range(n_samples)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))  # Optimized size for LaTeX\n",
        "\n",
        "    # Plot true values (black line)\n",
        "    ax.plot(x_axis, y_true[:n_samples], label='True Values',\n",
        "            linewidth=2, alpha=0.8, color='black')\n",
        "\n",
        "    # Plot predictions (colored line)\n",
        "    ax.plot(x_axis, y_pred[:n_samples], label=f'{model_name} Predictions',\n",
        "            linewidth=1.5, alpha=0.7, color=color)\n",
        "\n",
        "    ax.set_xlabel('Time Steps', fontsize=12)\n",
        "    ax.set_ylabel('CO Concentration (mg/mÂ³)', fontsize=12)\n",
        "    ax.set_title(f'{model_name} Model\\nRÂ² = {r2:.4f}', fontsize=13, fontweight='bold')\n",
        "    ax.legend(loc='upper right', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if filename provided\n",
        "    if filename:\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        print(f\"    Saved: {filename}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# Generate 4 Separate Time-Series Figures\n",
        "# ============================================================================\n",
        "\n",
        "print(\" Generating 4 separate time-series prediction figures...\\n\")\n",
        "\n",
        "# Figure 1: LSTM\n",
        "print(\"LSTM Time-Series Plot...\")\n",
        "plot_timeseries_single(\n",
        "    y_test_aligned,\n",
        "    lstm_pred,\n",
        "    'LSTM',\n",
        "    lstm_metrics['RÂ²'],\n",
        "    color='blue',\n",
        "    n_samples=500,\n",
        "    filename='fig_timeseries_lstm.png'\n",
        ")\n",
        "\n",
        "# Figure 2: Random Forest\n",
        "print(\"Random Forest Time-Series Plot...\")\n",
        "plot_timeseries_single(\n",
        "    y_test_aligned,\n",
        "    rf_pred,\n",
        "    'Random Forest',\n",
        "    rf_metrics['RÂ²'],\n",
        "    color='green',\n",
        "    n_samples=500,\n",
        "    filename='fig_timeseries_rf.png'\n",
        ")\n",
        "\n",
        "# Figure 3: Linear Regression\n",
        "print(\"Linear Regression Time-Series Plot...\")\n",
        "plot_timeseries_single(\n",
        "    y_test_aligned,\n",
        "    lr_pred,\n",
        "    'Linear Regression',\n",
        "    lr_metrics['RÂ²'],\n",
        "    color='red',\n",
        "    n_samples=500,\n",
        "    filename='fig_timeseries_lr.png'\n",
        ")\n",
        "\n",
        "# Figure 4: Ensemble\n",
        "print(\" Ensemble Time-Series Plot...\")\n",
        "plot_timeseries_single(\n",
        "    y_test_aligned,\n",
        "    ensemble_pred,\n",
        "    'Ensemble',\n",
        "    ensemble_metrics['RÂ²'],\n",
        "    color='purple',\n",
        "    n_samples=500,\n",
        "    filename='fig_timeseries_ensemble.png'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL 4 TIME-SERIES PLOTS GENERATED AND SAVED!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nFiles saved:\")\n",
        "print(\"  1. fig_timeseries_lstm.png\")\n",
        "print(\"  2. fig_timeseries_rf.png\")\n",
        "print(\"  3. fig_timeseries_lr.png\")\n",
        "print(\"  4. fig_timeseries_ensemble.png\")\n",
        "print(\"\\nYou can now right-click and save each image separately!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "POUXypA5m9sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Scatter Plots - True vs Predicted Values (SEPARATE FIGURES)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to plot scatter with regression line and legend\n",
        "def plot_scatter_single(y_true, y_pred, model_name, r2, filename=None):\n",
        "    \"\"\"\n",
        "    Create individual scatter plot for one model\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "    # Scatter plot with label\n",
        "    ax.scatter(y_true, y_pred, alpha=0.5, s=15, label='Predictions')\n",
        "\n",
        "    # Perfect prediction line (y=x)\n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "\n",
        "    ax.set_xlabel('True CO Concentration (mg/mÂ³)', fontsize=13)\n",
        "    ax.set_ylabel('Predicted CO Concentration (mg/mÂ³)', fontsize=13)\n",
        "    ax.set_title(f'{model_name} Model\\nRÂ² = {r2:.4f}', fontsize=14, fontweight='bold')\n",
        "    ax.legend(loc='upper left', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if filename provided\n",
        "    if filename:\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   Saved: {filename}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# Generate 4 Separate Figures\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Generating 4 separate scatter plot figures...\\n\")\n",
        "\n",
        "# Figure 1: LSTM\n",
        "print(\"LSTM Scatter Plot...\")\n",
        "plot_scatter_single(\n",
        "    y_test_aligned,\n",
        "    lstm_pred,\n",
        "    'LSTM',\n",
        "    lstm_metrics['RÂ²'],\n",
        "    filename='fig_scatter_lstm.png'\n",
        ")\n",
        "\n",
        "# Figure 2: Random Forest\n",
        "print(\"Random Forest Scatter Plot...\")\n",
        "plot_scatter_single(\n",
        "    y_test_aligned,\n",
        "    rf_pred,\n",
        "    'Random Forest',\n",
        "    rf_metrics['RÂ²'],\n",
        "    filename='fig_scatter_rf.png'\n",
        ")\n",
        "\n",
        "# Figure 3: Linear Regression\n",
        "print(\"Linear Regression Scatter Plot...\")\n",
        "plot_scatter_single(\n",
        "    y_test_aligned,\n",
        "    lr_pred,\n",
        "    'Linear Regression',\n",
        "    lr_metrics['RÂ²'],\n",
        "    filename='fig_scatter_lr.png'\n",
        ")\n",
        "\n",
        "# Figure 4: Ensemble\n",
        "print(\"Ensemble Scatter Plot...\")\n",
        "plot_scatter_single(\n",
        "    y_test_aligned,\n",
        "    ensemble_pred,\n",
        "    'Ensemble',\n",
        "    ensemble_metrics['RÂ²'],\n",
        "    filename='fig_scatter_ensemble.png'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL 4 SCATTER PLOTS GENERATED AND SAVED!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nFiles saved:\")\n",
        "print(\"  1. fig_scatter_lstm.png\")\n",
        "print(\"  2. fig_scatter_rf.png\")\n",
        "print(\"  3. fig_scatter_lr.png\")\n",
        "print(\"  4. fig_scatter_ensemble.png\")\n",
        "print(\"\\nYou can now right-click and save each image separately!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "DTELatZonIUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Error Analysis and Distribution (SEPARATE FIGURES)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Calculate residuals (errors)\n",
        "lstm_residuals = y_test_aligned - lstm_pred\n",
        "rf_residuals = y_test_aligned - rf_pred\n",
        "lr_residuals = y_test_aligned - lr_pred\n",
        "ensemble_residuals = y_test_aligned - ensemble_pred\n",
        "\n",
        "def plot_residuals_single(residuals, model_name, color='blue', filename=None):\n",
        "    \"\"\"\n",
        "    Create individual residual distribution plot for one model\n",
        "\n",
        "    Args:\n",
        "        residuals: Array of residual errors\n",
        "        model_name: Name of the model\n",
        "        color: Color for histogram\n",
        "        filename: Path to save figure\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Plot histogram\n",
        "    ax.hist(residuals, bins=50, alpha=0.7, color=color, edgecolor='black')\n",
        "\n",
        "    # Add vertical line at zero (perfect prediction)\n",
        "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "\n",
        "    # Calculate statistics\n",
        "    mean_error = residuals.mean()\n",
        "    std_error = residuals.std()\n",
        "    max_error = np.abs(residuals).max()\n",
        "\n",
        "    # Add statistics text box\n",
        "    stats_text = f'Mean: {mean_error:.3f} mg/mÂ³\\nStd Dev: {std_error:.3f} mg/mÂ³\\nMax Error: {max_error:.3f} mg/mÂ³'\n",
        "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
        "            fontsize=10, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    ax.set_xlabel('Residuals (mg/mÂ³)', fontsize=12)\n",
        "    ax.set_ylabel('Frequency', fontsize=12)\n",
        "    ax.set_title(f'{model_name} Residuals Distribution', fontsize=13, fontweight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if filename provided\n",
        "    if filename:\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        print(f\" Saved: {filename}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# Generate 4 Separate Residual Distribution Figures\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Generating 4 separate residual distribution figures...\\n\")\n",
        "\n",
        "# Figure 1: LSTM\n",
        "print(\"LSTM Residuals...\")\n",
        "plot_residuals_single(\n",
        "    lstm_residuals,\n",
        "    'LSTM',\n",
        "    color='blue',\n",
        "    filename='fig_residuals_lstm.png'\n",
        ")\n",
        "\n",
        "# Figure 2: Random Forest\n",
        "print(\"Random Forest Residuals...\")\n",
        "plot_residuals_single(\n",
        "    rf_residuals,\n",
        "    'Random Forest',\n",
        "    color='green',\n",
        "    filename='fig_residuals_rf.png'\n",
        ")\n",
        "\n",
        "# Figure 3: Linear Regression\n",
        "print(\"Linear Regression Residuals...\")\n",
        "plot_residuals_single(\n",
        "    lr_residuals,\n",
        "    'Linear Regression',\n",
        "    color='red',\n",
        "    filename='fig_residuals_lr.png'\n",
        ")\n",
        "\n",
        "# Figure 4: Ensemble\n",
        "print(\" Ensemble Residuals...\")\n",
        "plot_residuals_single(\n",
        "    ensemble_residuals,\n",
        "    'Ensemble',\n",
        "    color='purple',\n",
        "    filename='fig_residuals_ensemble.png'\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# Print Comprehensive Residual Statistics\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" RESIDUAL STATISTICS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Model':<20} {'Mean Error':<15} {'Std Dev':<15} {'Max Error':<15}\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'LSTM':<20} {lstm_residuals.mean():<15.4f} {lstm_residuals.std():<15.4f} {np.abs(lstm_residuals).max():<15.4f}\")\n",
        "print(f\"{'Random Forest':<20} {rf_residuals.mean():<15.4f} {rf_residuals.std():<15.4f} {np.abs(rf_residuals).max():<15.4f}\")\n",
        "print(f\"{'Linear Regression':<20} {lr_residuals.mean():<15.4f} {lr_residuals.std():<15.4f} {np.abs(lr_residuals).max():<15.4f}\")\n",
        "print(f\"{'Ensemble':<20} {ensemble_residuals.mean():<15.4f} {ensemble_residuals.std():<15.4f} {np.abs(ensemble_residuals).max():<15.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n Interpretation:\")\n",
        "print(\"â€¢ Mean Error near 0 = Unbiased predictions\")\n",
        "print(\"â€¢ Low Std Dev = Consistent accuracy\")\n",
        "print(\"â€¢ Symmetric distribution around 0 = No systematic bias\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL 4 RESIDUAL PLOTS GENERATED AND SAVED!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nFiles saved:\")\n",
        "print(\"  1. fig_residuals_lstm.png\")\n",
        "print(\"  2. fig_residuals_rf.png\")\n",
        "print(\"  3. fig_residuals_lr.png\")\n",
        "print(\"  4. fig_residuals_ensemble.png\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "6Dn8w6s4nSHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21: Bar Chart Comparison of Model Performance\n",
        "\n",
        "# Prepare data for visualization\n",
        "models = results_df['Model'].values\n",
        "r2_scores = results_df['RÂ²'].values\n",
        "rmse_scores = results_df['RMSE'].values\n",
        "mae_scores = results_df['MAE'].values\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Bar colors\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
        "\n",
        "# Plot RÂ² scores\n",
        "axes[0].bar(range(len(models)), r2_scores, color=colors)\n",
        "axes[0].set_xticks(range(len(models)))\n",
        "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0].set_ylabel('RÂ² Score', fontsize=12)\n",
        "axes[0].set_title('RÂ² Score Comparison\\n(Higher is Better)', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylim([0.8, 1.0])\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(r2_scores):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot RMSE\n",
        "axes[1].bar(range(len(models)), rmse_scores, color=colors)\n",
        "axes[1].set_xticks(range(len(models)))\n",
        "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1].set_ylabel('RMSE (mg/mÂ³)', fontsize=12)\n",
        "axes[1].set_title('RMSE Comparison\\n(Lower is Better)', fontsize=13, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(rmse_scores):\n",
        "    axes[1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot MAE\n",
        "axes[2].bar(range(len(models)), mae_scores, color=colors)\n",
        "axes[2].set_xticks(range(len(models)))\n",
        "axes[2].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[2].set_ylabel('MAE (mg/mÂ³)', fontsize=12)\n",
        "axes[2].set_title('MAE Comparison\\n(Lower is Better)', fontsize=13, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(mae_scores):\n",
        "    axes[2].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Bar chart comparison completed!\")"
      ],
      "metadata": {
        "id": "0UFqJ2dpncCL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7iZsIg5y4_Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7d4ff9a"
      },
      "source": [
        "# Task\n",
        "Load and parse the \"Air Quality Standards Table.csv\" file to extract AQI breakpoints for CO, C6H6, NO2, NOx, and O3, storing them in a structured format for easy lookup. Then, prepare the pollutant data by extracting predicted CO values from `ensemble_pred` and actual C6H6(GT), NO2(GT), NOx(GT), and PT08.S5(O3) values from `df_clean`, ensuring all data is aligned with `y_test_aligned` for subsequent AQI calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889d8dae"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will load the 'Air Quality Standards Table.csv' file into a pandas DataFrame called `aqi_standards_df` and then display its head, info, and columns to understand its structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "821e3ef2"
      },
      "source": [
        "## Load and Parse AQI Standards\n",
        "\n",
        "### Subtask:\n",
        "Load the 'Air Quality Standards Table.csv' and parse it into a structured format (e.g., dictionary or DataFrame) that allows easy lookup of AQI breakpoints for each specified pollutant (CO, C6H6, NO2, NOx, O3).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae021f7f"
      },
      "source": [
        "# Task\n",
        "# Plan:\n",
        "* **Add pandas import to the cell**: To make the selected code runnable in isolation, I will add `import pandas as pd` to the beginning of the cell. This ensures the `pd` alias is recognized when `pd.read_csv` is called.\n",
        "* **Execute the modified cell**: After adding the import statement, the cell will be executed to load the AQI standards table and display its initial information.\n",
        "* **Proceed with AQI parsing**: Once the DataFrame is successfully loaded, the next steps will involve parsing this DataFrame to extract the necessary AQI breakpoints for each pollutant.\n",
        "\n",
        "Load the 'Air Quality Standards Table.csv' file into a pandas DataFrame and display its head, info, and columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32046bb3"
      },
      "source": [
        "## Load and Parse AQI Standards\n",
        "\n",
        "### Subtask:\n",
        "Load the 'Air Quality Standards Table.csv' file into a pandas DataFrame and display its head, info, and columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc27d761"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the 'Air Quality Standards Table.csv' file into a pandas DataFrame and display its head, info, and columns as requested in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f8048f2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "aqi_standards_df = pd.read_csv('/content/Air Quality Standards Table.csv', encoding='latin-1')\n",
        "\n",
        "print(\"First 5 rows of AQI Standards Table:\")\n",
        "print(aqi_standards_df.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "aqi_standards_df.info()\n",
        "print(\"\\nColumn Names:\")\n",
        "print(aqi_standards_df.columns.tolist())\n",
        "\n",
        "# Display the full table with all rows and columns visible\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL AQI STANDARDS TABLE:\")\n",
        "print(\"=\"*80)\n",
        "pd.set_option('display.max_rows', None)  # Show all rows\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "pd.set_option('display.width', None)  # Auto-detect width\n",
        "pd.set_option('display.max_colwidth', None)  # Show full column content\n",
        "display(aqi_standards_df)  # Use display() for better formatting in notebooks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf784253"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output showed that the AQI standards table needs cleaning and reformatting to be useful. The first row contains the actual column names (pollutant names), and the data starts from the second row. I will set the first row as the new header, clean up the column names, and then parse the AQI ranges and pollutant concentration ranges into a structured format for each pollutant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bcd4e22"
      },
      "source": [
        "# Task\n",
        "Clean the `aqi_standards_df` by setting the first row as the header, renaming columns, and then parsing the AQI and pollutant concentration ranges for CO, C6H6, NO2, NOx, and O3. Store these parsed breakpoints in a structured dictionary named `aqi_breakpoints` for easy lookup. Finally, verify the structure of `aqi_breakpoints`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51021a95"
      },
      "source": [
        "## Clean the aqi_standards_df and parse AQI breakpoints\n",
        "\n",
        "### Subtask:\n",
        "Clean the `aqi_standards_df` by setting the first row as the header, renaming columns, and parsing AQI and pollutant concentration ranges for CO, C6H6, NO2, NOx, and O3 into a structured dictionary `aqi_breakpoints`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d8bfb50"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires cleaning the AQI standards DataFrame, setting the correct header, renaming columns, and parsing the AQI and pollutant concentration ranges into a structured dictionary. This code block will perform all these steps by defining helper functions for parsing ranges and then iterating through the cleaned DataFrame to populate the `aqi_breakpoints` dictionary as specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21df2d07"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# 1. Create a copy of the aqi_standards_df and name it aqi_df_cleaned.\n",
        "aqi_df_cleaned = aqi_standards_df.copy()\n",
        "\n",
        "# 2. Set the first row (index 0) of aqi_df_cleaned as the new column headers.\n",
        "aqi_df_cleaned.columns = aqi_df_cleaned.iloc[0]\n",
        "\n",
        "# 3. Remove the first row from aqi_df_cleaned and reset the index.\n",
        "aqi_df_cleaned = aqi_df_cleaned[1:].reset_index(drop=True)\n",
        "\n",
        "# Print columns to see what we have\n",
        "print(\"=\" * 80)\n",
        "print(\"ACTUAL COLUMNS IN THE DATAFRAME:\")\n",
        "print(\"=\" * 80)\n",
        "for i, col in enumerate(aqi_df_cleaned.columns):\n",
        "    print(f\"{i}: '{col}' (type: {type(col).__name__})\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display the dataframe to see the data\n",
        "print(\"First few rows:\")\n",
        "print(aqi_df_cleaned.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# 4. Create a more flexible renaming map by searching for keywords\n",
        "new_column_names = {}\n",
        "for col in aqi_df_cleaned.columns:\n",
        "    col_str = str(col).strip()\n",
        "\n",
        "    # Check for each column type\n",
        "    if pd.isna(col) or col_str == 'nan' or col_str == '':\n",
        "        new_column_names[col] = 'AQI Category'\n",
        "    elif 'AQI' in col_str and ('Range' in col_str or 'Values' in col_str):\n",
        "        new_column_names[col] = 'AQI Range'\n",
        "    elif 'CO' in col_str and 'Carbon' in col_str:\n",
        "        new_column_names[col] = 'CO'\n",
        "    elif 'C6H6' in col_str or 'Benzene' in col_str:\n",
        "        new_column_names[col] = 'C6H6'\n",
        "    elif 'NO2' in col_str and 'Nitrogen Dioxide' in col_str:\n",
        "        new_column_names[col] = 'NO2'\n",
        "    elif 'NOx' in col_str or 'Nitrogen Oxides' in col_str:\n",
        "        new_column_names[col] = 'NOx'\n",
        "    elif 'O3' in col_str or 'Ozone' in col_str:\n",
        "        new_column_names[col] = 'O3'\n",
        "    elif 'Health' in col_str:\n",
        "        new_column_names[col] = 'Health Implications'\n",
        "    elif 'Message' in col_str or 'Cautionary' in col_str:\n",
        "        new_column_names[col] = 'Message'\n",
        "    else:\n",
        "        new_column_names[col] = col  # Keep original if no match\n",
        "\n",
        "print(\"Column Renaming Map:\")\n",
        "for old, new in new_column_names.items():\n",
        "    print(f\"  '{old}' -> '{new}'\")\n",
        "print(\"\\n\")\n",
        "\n",
        "aqi_df_cleaned = aqi_df_cleaned.rename(columns=new_column_names)\n",
        "\n",
        "print(\"Columns after renaming:\")\n",
        "print(aqi_df_cleaned.columns.tolist())\n",
        "print(\"\\n\")\n",
        "\n",
        "# 5. Drop the columns 'Health Implications' and 'Message' only if they exist\n",
        "columns_to_drop = ['Health Implications', 'Message']\n",
        "columns_to_drop = [col for col in columns_to_drop if col in aqi_df_cleaned.columns]\n",
        "\n",
        "if columns_to_drop:\n",
        "    aqi_df_cleaned = aqi_df_cleaned.drop(columns=columns_to_drop)\n",
        "    print(f\"Dropped columns: {columns_to_drop}\")\n",
        "else:\n",
        "    print(\"No 'Health Implications' or 'Message' columns found to drop\")\n",
        "\n",
        "print(\"\\nRemaining columns:\")\n",
        "print(aqi_df_cleaned.columns.tolist())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check if required columns exist\n",
        "required_columns = ['AQI Category', 'AQI Range', 'CO', 'C6H6', 'NO2', 'NOx', 'O3']\n",
        "missing_columns = [col for col in required_columns if col not in aqi_df_cleaned.columns]\n",
        "\n",
        "if missing_columns:\n",
        "    print(f\"WARNING: Missing required columns: {missing_columns}\")\n",
        "    print(\"Please check the column renaming map above and adjust accordingly.\")\n",
        "else:\n",
        "    print(\"All required columns are present!\")\n",
        "\n",
        "    # 6. Define a helper function parse_concentration_range\n",
        "    def parse_concentration_range(range_str, pollutant_name):\n",
        "        range_str = str(range_str).strip()\n",
        "        lower = 0.0\n",
        "        upper = np.inf\n",
        "\n",
        "        if range_str.startswith('>'):\n",
        "            num_part = re.findall(r'\\d+\\.?\\d*', range_str)[0]\n",
        "            lower = float(num_part)\n",
        "        else:\n",
        "            parts = re.findall(r'\\d+\\.?\\d*', range_str)\n",
        "            if len(parts) == 1:\n",
        "                lower = float(parts[0])\n",
        "                upper = float(parts[0])\n",
        "            elif len(parts) == 2:\n",
        "                lower = float(parts[0])\n",
        "                upper = float(parts[1])\n",
        "            elif len(parts) > 2:\n",
        "                lower = float(parts[0])\n",
        "                upper = float(parts[1])\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        if pollutant_name in ['C6H6', 'NO2', 'NOx', 'O3']:\n",
        "            lower /= 1000.0\n",
        "            if upper != np.inf:\n",
        "                upper /= 1000.0\n",
        "\n",
        "        return (lower, upper)\n",
        "\n",
        "    # 7. Define a helper function parse_aqi_range\n",
        "    def parse_aqi_range(aqi_range_str):\n",
        "        aqi_range_str = str(aqi_range_str).strip()\n",
        "        lower = 0\n",
        "        upper = np.inf\n",
        "\n",
        "        if aqi_range_str.startswith('>'):\n",
        "            lower = int(re.findall(r'\\d+', aqi_range_str)[0])\n",
        "        else:\n",
        "            parts = aqi_range_str.split('-')\n",
        "            if len(parts) == 2:\n",
        "                lower = int(parts[0])\n",
        "                upper = int(parts[1])\n",
        "            elif len(parts) == 1 and parts[0].isdigit():\n",
        "                lower = int(parts[0])\n",
        "                upper = int(parts[0])\n",
        "            else:\n",
        "                return None\n",
        "        return (lower, upper)\n",
        "\n",
        "    # 8. Initialize an empty dictionary named aqi_breakpoints\n",
        "    aqi_breakpoints = {\n",
        "        'CO': [], 'C6H6': [], 'NO2': [], 'NOx': [], 'O3': []\n",
        "    }\n",
        "\n",
        "    # 9. Iterate through the rows and populate aqi_breakpoints\n",
        "    for index, row in aqi_df_cleaned.iterrows():\n",
        "        aqi_category = row['AQI Category']\n",
        "        aqi_range_parsed = parse_aqi_range(row['AQI Range'])\n",
        "\n",
        "        if aqi_range_parsed is None:\n",
        "            continue\n",
        "\n",
        "        for pollutant in ['CO', 'C6H6', 'NO2', 'NOx', 'O3']:\n",
        "            concentration_range_str = row[pollutant]\n",
        "            concentration_range_parsed = parse_concentration_range(concentration_range_str, pollutant)\n",
        "\n",
        "            if concentration_range_parsed is None:\n",
        "                continue\n",
        "\n",
        "            aqi_breakpoints[pollutant].append({\n",
        "                'aqi_category': aqi_category,\n",
        "                'aqi_range': aqi_range_parsed,\n",
        "                'concentration_range': concentration_range_parsed\n",
        "            })\n",
        "\n",
        "    # 10. Print the aqi_breakpoints dictionary\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"AQI BREAKPOINTS DICTIONARY:\")\n",
        "    print(\"=\" * 80)\n",
        "    import json\n",
        "    print(json.dumps(aqi_breakpoints, indent=4, default=lambda x: str(x) if isinstance(x, (np.ndarray, np.generic)) else x))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CLEANED AQI DATAFRAME:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(aqi_df_cleaned.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b65e7c90"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to rename columns failed because the `aqi_df_cleaned.columns = aqi_df_cleaned.iloc[0]` step inadvertently assigned `NaN` values as column names due to the structure of the input CSV. The `pd.read_csv` correctly parsed the header of the file, but the first *data row* contained `NaN`s in the 'AQI Category' and 'AQI Range' columns, which was then mistakenly used as the new header. I need to correct this by directly using the column names loaded by `pd.read_csv`, dropping the first data row (index 0), and then renaming the descriptive column names to the shorter pollutant codes (CO, C6H6, etc.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6276374f"
      },
      "source": [
        "import numpy as np\n",
        "import re # For regular expressions to parse strings\n",
        "\n",
        "# 1. Create a copy of the aqi_standards_df. It already has the correct initial column names.\n",
        "aqi_df_cleaned = aqi_standards_df.copy()\n",
        "\n",
        "# 2. Remove the first data row (index 0) as it contains NaNs for AQI Category and AQI Range.\n",
        "# The actual data starts from index 1.\n",
        "aqi_df_cleaned = aqi_df_cleaned[1:].reset_index(drop=True)\n",
        "\n",
        "# 3. Rename the relevant columns to shorter, more usable names.\n",
        "# These are the actual column names loaded by pd.read_csv from the header row.\n",
        "column_rename_map = {\n",
        "    'AQI Category': 'AQI Category',\n",
        "    'AQI Range': 'AQI Range',\n",
        "    'Carbon Monoxide (CO) Concentration (8-hour avg)': 'CO',\n",
        "    'Benzene (C6H6) Concentration (annual avg)': 'C6H6',\n",
        "    'Nitrogen Dioxide (NO2) Concentration (?g/mÂ³)': 'NO2', # Note: ?g/mÂ³ is Âµg/mÂ³\n",
        "    'Nitrogen Oxides (NOx) Concentration (annual avg)': 'NOx', # Note: ?g/mÂ³ is Âµg/mÂ³\n",
        "    'Ozone (O3) Concentration (8-hour avg)': 'O3',\n",
        "    'Health Implications': 'Health Implications',\n",
        "    'Message': 'Message'\n",
        "}\n",
        "aqi_df_cleaned = aqi_df_cleaned.rename(columns=column_rename_map)\n",
        "\n",
        "print(\"Columns after initial renaming:\")\n",
        "print(aqi_df_cleaned.columns.tolist())\n",
        "print(\"\\n\")\n",
        "\n",
        "# 4. Drop the columns 'Health Implications' and 'Message' as they are not needed for breakpoint lookup.\n",
        "columns_to_drop = ['Health Implications', 'Message']\n",
        "# Ensure columns exist before dropping to prevent errors\n",
        "columns_to_drop = [col for col in columns_to_drop if col in aqi_df_cleaned.columns]\n",
        "\n",
        "if columns_to_drop:\n",
        "    aqi_df_cleaned = aqi_df_cleaned.drop(columns=columns_to_drop)\n",
        "    print(f\"Dropped columns: {columns_to_drop}\")\n",
        "else:\n",
        "    print(\"No 'Health Implications' or 'Message' columns found to drop.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Final columns in aqi_df_cleaned:\")\n",
        "print(aqi_df_cleaned.columns.tolist())\n",
        "print(\"\\n\")\n",
        "\n",
        "# 5. Define a helper function parse_concentration_range\n",
        "def parse_concentration_range(range_str, pollutant_name):\n",
        "    range_str = str(range_str).strip() # Ensure it's a string and remove whitespace\n",
        "    lower = 0.0\n",
        "    upper = np.inf\n",
        "\n",
        "    # Handle cases where range_str might be 'NaN' or empty\n",
        "    if range_str == 'nan' or not range_str:\n",
        "        return None\n",
        "\n",
        "    # Handle '>' cases first, assuming no '<' cases\n",
        "    if range_str.startswith('>'):\n",
        "        num_part = re.findall(r'\\d+\\.?\\d*', range_str)[0]\n",
        "        lower = float(num_part)\n",
        "    else:\n",
        "        # Use a more robust regex to find numerical parts that might be separated by '-' or ' ' or just one number\n",
        "        parts = re.findall(r'\\d+\\.?\\d*', range_str)\n",
        "        if len(parts) == 1: # Single value, e.g., '4.4'\n",
        "            lower = float(parts[0])\n",
        "            upper = float(parts[0]) # For exact values, though often it's a range\n",
        "        elif len(parts) >= 2: # 'min-max' or more numbers (take first two as min/max)\n",
        "            lower = float(parts[0])\n",
        "            upper = float(parts[1])\n",
        "        else:\n",
        "            return None # Handle cases like unexpected format\n",
        "\n",
        "    # Convert units to mg/mÂ³ where necessary\n",
        "    # C6H6, NO2, NOx, O3 are in Âµg/mÂ³ and need to be converted to mg/mÂ³\n",
        "    if pollutant_name in ['C6H6', 'NO2', 'NOx', 'O3']:\n",
        "        lower /= 1000.0\n",
        "        if upper != np.inf:\n",
        "            upper /= 1000.0\n",
        "\n",
        "    # For CO, the unit is already mg/mÂ³, no conversion needed.\n",
        "\n",
        "    return (lower, upper)\n",
        "\n",
        "# 6. Define a helper function parse_aqi_range\n",
        "def parse_aqi_range(aqi_range_str):\n",
        "    aqi_range_str = str(aqi_range_str).strip()\n",
        "    lower = 0\n",
        "    upper = np.inf\n",
        "\n",
        "    if aqi_range_str == 'nan' or not aqi_range_str:\n",
        "        return None\n",
        "\n",
        "    if aqi_range_str.startswith('>'):\n",
        "        lower = int(re.findall(r'\\d+', aqi_range_str)[0])\n",
        "    else:\n",
        "        parts = aqi_range_str.split('-')\n",
        "        if len(parts) == 2:\n",
        "            lower = int(parts[0])\n",
        "            upper = int(parts[1])\n",
        "        elif len(parts) == 1 and parts[0].isdigit(): # Single number cases, though not expected from table\n",
        "            lower = int(parts[0])\n",
        "            upper = int(parts[0])\n",
        "        else:\n",
        "            return None # Handle cases with unexpected format or NaN\n",
        "    return (lower, upper)\n",
        "\n",
        "# 7. Initialize an empty dictionary named aqi_breakpoints\n",
        "aqi_breakpoints = {\n",
        "    'CO': [], 'C6H6': [], 'NO2': [], 'NOx': [], 'O3': []\n",
        "}\n",
        "\n",
        "# 8. Iterate through the rows and populate aqi_breakpoints\n",
        "for index, row in aqi_df_cleaned.iterrows():\n",
        "    aqi_category = row['AQI Category']\n",
        "    aqi_range_parsed = parse_aqi_range(row['AQI Range'])\n",
        "\n",
        "    if aqi_range_parsed is None:\n",
        "        continue # Skip rows that couldn't be parsed (e.g. if AQI Range is NaN)\n",
        "\n",
        "    for pollutant in ['CO', 'C6H6', 'NO2', 'NOx', 'O3']:\n",
        "        # Check if the pollutant column exists before trying to access it\n",
        "        if pollutant in aqi_df_cleaned.columns:\n",
        "            concentration_range_str = row[pollutant]\n",
        "            concentration_range_parsed = parse_concentration_range(concentration_range_str, pollutant)\n",
        "\n",
        "            if concentration_range_parsed is None:\n",
        "                continue # Skip if concentration range couldn't be parsed\n",
        "\n",
        "            aqi_breakpoints[pollutant].append({\n",
        "                'aqi_category': aqi_category,\n",
        "                'aqi_range': aqi_range_parsed,\n",
        "                'concentration_range': concentration_range_parsed\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Warning: Pollutant column '{pollutant}' not found in aqi_df_cleaned. Skipping.\")\n",
        "\n",
        "# 9. Print the aqi_breakpoints dictionary\n",
        "print(\"AQI Breakpoints Dictionary:\")\n",
        "import json\n",
        "print(json.dumps(aqi_breakpoints, indent=4, default=lambda x: str(x) if isinstance(x, (np.ndarray, np.generic)) else x))\n",
        "\n",
        "print(\"\\nCleaned AQI DataFrame head:\")\n",
        "print(aqi_df_cleaned.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09709353"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires extracting specific pollutant data (predicted CO and actual C6H6, NO2, NOx, O3) from existing variables and aligning them with the `y_test_aligned` data for subsequent AQI calculations. This step will gather all necessary pollutant concentrations into a single structured format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preparing pollutant data for AQI calculation...\")\n",
        "\n",
        "# The start and end indices for the aligned test data in the original df_clean\n",
        "# This aligns with the length of ensemble_pred, which is already aligned with the test set.\n",
        "# The offset comes from SEQUENCE_LENGTH because LSTM predictions start after the initial sequence length.\n",
        "start_idx_aligned = val_end + SEQUENCE_LENGTH\n",
        "end_idx_aligned = start_idx_aligned + len(ensemble_pred)\n",
        "\n",
        "# Extract actual pollutant values from df_clean for the aligned test period\n",
        "actual_pollutants_aligned = df_clean.iloc[start_idx_aligned:end_idx_aligned].copy()\n",
        "\n",
        "# Create a dictionary to store all pollutant concentrations (predicted and actual) aligned\n",
        "pollutant_data = {\n",
        "    'CO': actual_pollutants_aligned['CO(GT)'].values,\n",
        "    'C6H6': actual_pollutants_aligned['C6H6(GT)'].values,\n",
        "    'NO2': actual_pollutants_aligned['NO2(GT)'].values,\n",
        "    'NOx': actual_pollutants_aligned['NOx(GT)'].values,\n",
        "    'O3': actual_pollutants_aligned['PT08.S5(O3)'].values\n",
        "}\n",
        "\n",
        "# Verify shapes of all pollutant data to ensure alignment\n",
        "print(f\"\\nShape of CO: {pollutant_data['CO'].shape}\")\n",
        "print(f\"Shape of C6H6: {pollutant_data['C6H6'].shape}\")\n",
        "print(f\"Shape of NO2: {pollutant_data['NO2'].shape}\")\n",
        "print(f\"Shape of NOx: {pollutant_data['NOx'].shape}\")\n",
        "print(f\"Shape of O3: {pollutant_data['O3'].shape}\")\n",
        "print(f\"Expected aligned length (ensemble_pred): {len(ensemble_pred)}\")\n",
        "\n",
        "# Display first few values for verification\n",
        "print(\"\\nFirst 5 aligned pollutant data points:\")\n",
        "for pollutant, values in pollutant_data.items():\n",
        "    print(f\"  {pollutant}: {values[:5]}\")\n",
        "\n",
        "print(\"Pollutant data prepared successfully!\")"
      ],
      "metadata": {
        "id": "OnV6BzcrPy2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWG5YyfAXuvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calculating AQI for each pollutant...\")\n",
        "\n",
        "def calculate_individual_aqi(concentration, pollutant_name, aqi_breakpoints):\n",
        "    \"\"\"\n",
        "    Calculates the AQI for a single pollutant concentration using linear interpolation.\n",
        "\n",
        "    Args:\n",
        "        concentration (float): The pollutant concentration.\n",
        "        pollutant_name (str): The name of the pollutant (e.g., 'CO', 'C6H6').\n",
        "        aqi_breakpoints (dict): Dictionary containing AQI breakpoint ranges.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated AQI value, or np.nan if concentration is invalid.\n",
        "    \"\"\"\n",
        "    if pd.isna(concentration):\n",
        "        return np.nan\n",
        "\n",
        "    breakpoints = aqi_breakpoints.get(pollutant_name)\n",
        "    if not breakpoints:\n",
        "        return np.nan\n",
        "\n",
        "    # Sort breakpoints by concentration_range lower bound to ensure correct iteration\n",
        "    breakpoints = sorted(breakpoints, key=lambda x: x['concentration_range'][0])\n",
        "\n",
        "    # Handle concentrations below the lowest range\n",
        "    if concentration <= breakpoints[0]['concentration_range'][0]:\n",
        "        return breakpoints[0]['aqi_range'][0]\n",
        "\n",
        "    # Handle concentrations above the highest range if upper limit is infinity\n",
        "    # This is a general approach for open-ended highest category\n",
        "    if breakpoints[-1]['concentration_range'][1] == np.inf and concentration > breakpoints[-1]['concentration_range'][0]:\n",
        "        return breakpoints[-1]['aqi_range'][1]\n",
        "\n",
        "    for i in range(len(breakpoints) - 1):\n",
        "        bp_low = breakpoints[i]\n",
        "        bp_high = breakpoints[i+1]\n",
        "\n",
        "        conc_low, conc_high = bp_low['concentration_range']\n",
        "        aqi_low, aqi_high = bp_low['aqi_range']\n",
        "\n",
        "        # Adjust conc_high to be just before the next category starts if they are contiguous\n",
        "        # This ensures proper range assignment for boundary values.\n",
        "        if i < len(breakpoints) - 1:\n",
        "            next_conc_low = breakpoints[i+1]['concentration_range'][0]\n",
        "            if conc_high >= next_conc_low: # If ranges overlap or touch\n",
        "                conc_high = next_conc_low - 1e-9 # A very small epsilon\n",
        "\n",
        "        if conc_low <= concentration <= conc_high:\n",
        "            # Linear interpolation formula: I = [(I_high - I_low) / (C_high - C_low)] * (C - C_low) + I_low\n",
        "            # Ensure C_high - C_low is not zero\n",
        "            if (conc_high - conc_low) == 0: # Handle single point ranges if they occur\n",
        "                return float(aqi_low) # Ensure float return\n",
        "            aqi = ((aqi_high - aqi_low) / (conc_high - conc_low)) * (concentration - conc_low) + aqi_low\n",
        "            return round(aqi, 2)\n",
        "\n",
        "    # If concentration falls within the last defined finite range (not caught by loop for highest inf)\n",
        "    last_bp = breakpoints[-1]\n",
        "    conc_low, conc_high = last_bp['concentration_range']\n",
        "    aqi_low, aqi_high = last_bp['aqi_range']\n",
        "    if conc_high != np.inf and conc_low <= concentration <= conc_high:\n",
        "        if (conc_high - conc_low) == 0:\n",
        "             return float(aqi_low) # Ensure float return\n",
        "        aqi = ((aqi_high - aqi_low) / (conc_high - conc_low)) * (concentration - conc_low) + aqi_low\n",
        "        return round(aqi, 2)\n",
        "\n",
        "    return np.nan # Should not be reached for valid concentrations covering all ranges\n",
        "\n",
        "\n",
        "aqi_values = {}\n",
        "for pollutant, concentrations in pollutant_data.items():\n",
        "    # Explicitly specify output type as float to handle NaNs and interpolated values\n",
        "    vectorized_aqi_calc = np.vectorize(lambda c: calculate_individual_aqi(c, pollutant, aqi_breakpoints), otypes=[np.float64])\n",
        "    aqis = vectorized_aqi_calc(concentrations)\n",
        "    aqi_values[pollutant] = aqis\n",
        "\n",
        "print(\"AQI values calculated successfully!\")\n",
        "print(\"\\nFirst 5 AQI values for each pollutant:\")\n",
        "for pollutant, aqis in aqi_values.items():\n",
        "    print(f\"  {pollutant}: {aqis[:5]}\")\n",
        "\n",
        "# Verify shapes\n",
        "print(\"\\nVerifying shapes of calculated AQI values:\")\n",
        "for pollutant, aqis in aqi_values.items():\n",
        "    print(f\"  {pollutant} AQI shape: {aqis.shape}\")"
      ],
      "metadata": {
        "id": "w72Skr5HRiJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cO-cS8kTR_f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd # Import pandas for pd.isna\n",
        "\n",
        "print(\"Defining the anomaly detection function...\")\n",
        "\n",
        "def detect_anomalies_in_aqi_series(aqi_series, sustained_period=3):\n",
        "    \"\"\"\n",
        "    Detects anomalies in an AQI time series based on predefined rules.\n",
        "\n",
        "    Args:\n",
        "        aqi_series (np.ndarray or list): The AQI time series (numerical values).\n",
        "        sustained_period (int): Number of consecutive hours an AQI must be above 200\n",
        "                                for a 'Moderate Anomaly' (default: 3).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of strings indicating the anomaly type for each time step.\n",
        "    \"\"\"\n",
        "    n_steps = len(aqi_series)\n",
        "    anomaly_types = np.full(n_steps, 'None', dtype='<U10') # Max length 'Emergency'\n",
        "\n",
        "    # Helper to track consecutive high AQI for 'Moderate Anomaly'\n",
        "    consecutive_high_aqi_count = 0\n",
        "\n",
        "    for i in range(n_steps):\n",
        "        current_aqi = aqi_series[i]\n",
        "        prev_aqi = aqi_series[i-1] if i > 0 else np.nan # Use NaN for first step\n",
        "\n",
        "        # Skip anomaly detection if current AQI is NaN\n",
        "        if pd.isna(current_aqi):\n",
        "            anomaly_types[i] = 'N/A'\n",
        "            consecutive_high_aqi_count = 0 # Reset count for NaN values\n",
        "            continue\n",
        "\n",
        "        # Reset consecutive count if AQI drops below 200 or is NaN\n",
        "        if current_aqi <= 200:\n",
        "            consecutive_high_aqi_count = 0\n",
        "        else:\n",
        "            consecutive_high_aqi_count += 1\n",
        "\n",
        "        # Apply rules in descending order of severity\n",
        "\n",
        "        # 1. Emergency\n",
        "        if current_aqi > 400:\n",
        "            anomaly_types[i] = 'Emergency'\n",
        "            continue # Move to next time step, this is the most severe\n",
        "\n",
        "        # 2. Severe Anomaly\n",
        "        # Check if prev_aqi is not NaN before calculating difference\n",
        "        if current_aqi > 300 or (not pd.isna(prev_aqi) and (current_aqi - prev_aqi) > 100):\n",
        "            anomaly_types[i] = 'Severe'\n",
        "            continue # Move to next time step, this is next most severe\n",
        "\n",
        "        # 3. Moderate Anomaly\n",
        "        # Check if current_aqi > 200 AND it has been for sustained_period consecutive hours\n",
        "        if current_aqi > 200 and consecutive_high_aqi_count >= sustained_period:\n",
        "            # Need to look back to confirm previous `sustained_period - 1` hours were also > 200\n",
        "            is_sustained = True\n",
        "            for j in range(1, sustained_period):\n",
        "                if i - j < 0 or aqi_series[i-j] <= 200 or pd.isna(aqi_series[i-j]):\n",
        "                    is_sustained = False\n",
        "                    break\n",
        "            if is_sustained:\n",
        "                anomaly_types[i] = 'Moderate'\n",
        "                continue # Move to next time step\n",
        "\n",
        "        # 4. Minor Anomaly\n",
        "        if not pd.isna(prev_aqi) and (current_aqi - prev_aqi) > 50:\n",
        "            anomaly_types[i] = 'Minor'\n",
        "\n",
        "    print(\"Anomaly detection function defined.\")\n",
        "    return anomaly_types"
      ],
      "metadata": {
        "id": "aeC1T1joS7UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying anomaly detection to individual pollutant AQIs...\")\n",
        "\n",
        "# 1. Initialize an empty dictionary named individual_aqi_anomalies\n",
        "individual_aqi_anomalies = {}\n",
        "\n",
        "# 2. Iterate through each pollutant and its corresponding AQI series\n",
        "for pollutant, aqi_series in aqi_values.items():\n",
        "    # 3. Call the detect_anomalies_in_aqi_series function\n",
        "    anomalies = detect_anomalies_in_aqi_series(aqi_series, sustained_period=3)\n",
        "\n",
        "    # Store the results\n",
        "    individual_aqi_anomalies[pollutant] = anomalies\n",
        "\n",
        "print(\"\\nIndividual AQI anomalies detected successfully!\")\n",
        "\n",
        "# 4. After processing all pollutants, iterate through the individual_aqi_anomalies dictionary\n",
        "print(\"\\nSummary of Individual Pollutant AQI Anomalies:\")\n",
        "print(\"=\" * 60)\n",
        "for pollutant, anomalies_array in individual_aqi_anomalies.items():\n",
        "    # 5. Calculate the counts of each unique anomaly type\n",
        "    unique_anomalies, counts = np.unique(anomalies_array, return_counts=True)\n",
        "    anomaly_counts = dict(zip(unique_anomalies, counts))\n",
        "\n",
        "    # Ensure all expected anomaly types are present for consistent printing\n",
        "    all_anomaly_types = ['None', 'Minor', 'Moderate', 'Severe', 'Emergency', 'N/A'] # Added 'N/A' to cover all possibilities\n",
        "    for anomaly_type in all_anomaly_types:\n",
        "        anomaly_counts.setdefault(anomaly_type, 0)\n",
        "\n",
        "    # 6. Print a formatted summary for each pollutant\n",
        "    print(f\"Pollutant: {pollutant}\")\n",
        "    for anomaly_type in all_anomaly_types:\n",
        "        print(f\"  {anomaly_type:<10}: {anomaly_counts[anomaly_type]:>7}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "print(\"Anomaly summary completed!\")"
      ],
      "metadata": {
        "id": "2FhApPuNUoH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying anomaly detection to individual pollutant AQIs...\")\n",
        "\n",
        "# 1. Initialize an empty dictionary named individual_aqi_anomalies\n",
        "individual_aqi_anomalies = {}\n",
        "\n",
        "# 2. Iterate through each pollutant and its corresponding AQI series\n",
        "for pollutant, aqi_series in aqi_values.items():\n",
        "    # 3. Call the detect_anomalies_in_aqi_series function\n",
        "    anomalies = detect_anomalies_in_aqi_series(aqi_series, sustained_period=3)\n",
        "\n",
        "    # Store the results\n",
        "    individual_aqi_anomalies[pollutant] = anomalies\n",
        "\n",
        "print(\"\\nIndividual AQI anomalies detected successfully!\")\n",
        "\n",
        "# 4. After processing all pollutants, iterate through the individual_aqi_anomalies dictionary\n",
        "print(\"\\nSummary of Individual Pollutant AQI Anomalies:\")\n",
        "print(\"=\" * 60)\n",
        "for pollutant, anomalies_array in individual_aqi_anomalies.items():\n",
        "    # 5. Calculate the counts of each unique anomaly type\n",
        "    unique_anomalies, counts = np.unique(anomalies_array, return_counts=True)\n",
        "    anomaly_counts = dict(zip(unique_anomalies, counts))\n",
        "\n",
        "    # Ensure all expected anomaly types are present for consistent printing\n",
        "    all_anomaly_types = ['None', 'Minor', 'Moderate', 'Severe', 'Emergency', 'N/A']\n",
        "    for anomaly_type in all_anomaly_types:\n",
        "        anomaly_counts.setdefault(anomaly_type, 0)\n",
        "\n",
        "    # 6. Print a formatted summary for each pollutant\n",
        "    print(f\"Pollutant: {pollutant}\")\n",
        "    for anomaly_type in all_anomaly_types:\n",
        "        print(f\"  {anomaly_type:<10}: {anomaly_counts[anomaly_type]:>7}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "print(\"Anomaly summary completed!\")"
      ],
      "metadata": {
        "id": "MkjtweV4XACE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import re # For parsing AQI ranges\n",
        "\n",
        "print(\"Generating graphs for each pollutant showing AQI and actual concentrations with categories...\")\n",
        "\n",
        "# Define the pollutants to visualize\n",
        "pollutants_to_visualize = ['CO', 'C6H6', 'NO2', 'NOx', 'O3']\n",
        "\n",
        "# Get the DateTime index corresponding to the aligned test data\n",
        "# Ensure start_idx_aligned and end_idx_aligned are defined (from pollutant_data preparation)\n",
        "time_index_aligned = df_clean['DateTime'].iloc[start_idx_aligned:end_idx_aligned].reset_index(drop=True)\n",
        "\n",
        "# Define a color palette for AQI categories for better visualization\n",
        "aqi_colors = {\n",
        "    'Good': 'green',\n",
        "    'Satisfactory': 'yellowgreen',\n",
        "    'Moderate': 'orange',\n",
        "    'Poor': 'red',\n",
        "    'Very Poor': 'purple',\n",
        "    'Severe': 'darkred'\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(len(pollutants_to_visualize), 1, figsize=(18, 7 * len(pollutants_to_visualize)), sharex=True)\n",
        "fig.suptitle('Individual Pollutant Concentrations and AQI by Category Over Time', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "# Ensure axes is an array even for a single subplot\n",
        "if len(pollutants_to_visualize) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Pre-parse AQI category ranges for plotting\n",
        "aqi_category_ranges = {}\n",
        "for _, row in aqi_df_cleaned.iterrows():\n",
        "    category = row['AQI Category']\n",
        "    aqi_range_str = str(row['AQI Range']).strip()\n",
        "\n",
        "    aqi_lower, aqi_upper = (0, np.inf) # Default values\n",
        "    if '-' in aqi_range_str:\n",
        "        parts = aqi_range_str.split('-')\n",
        "        aqi_lower = int(parts[0])\n",
        "        aqi_upper = int(parts[1])\n",
        "    elif '>' in aqi_range_str:\n",
        "        aqi_lower = int(re.findall(r'\\d+', aqi_range_str)[0])\n",
        "        aqi_upper = np.inf # Open-ended upper bound\n",
        "\n",
        "    aqi_category_ranges[category] = {'lower': aqi_lower, 'upper': aqi_upper}\n",
        "\n",
        "# Sort categories by their lower bounds for consistent plotting order\n",
        "sorted_categories = sorted(aqi_category_ranges.items(), key=lambda item: item[1]['lower'])\n",
        "\n",
        "for i, pollutant in enumerate(pollutants_to_visualize):\n",
        "    ax1 = axes[i]\n",
        "\n",
        "    # --- Plot pollutant concentration on left y-axis ---\n",
        "    sns.lineplot(\n",
        "        x=time_index_aligned,\n",
        "        y=pollutant_data[pollutant],\n",
        "        ax=ax1,\n",
        "        color='blue',\n",
        "        label=f'{pollutant} Concentration',\n",
        "        linewidth=1.5,\n",
        "        alpha=0.7\n",
        "    )\n",
        "    ax1.set_ylabel(f'{pollutant} Concentration', color='blue', fontsize=12, fontweight='bold')\n",
        "    ax1.tick_params(axis='y', labelcolor='blue')\n",
        "    ax1.legend(loc='upper left', bbox_to_anchor=(0.0, 1.0))\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Create a second y-axis for AQI ---\n",
        "    ax2 = ax1.twinx()\n",
        "    sns.lineplot(\n",
        "        x=time_index_aligned,\n",
        "        y=aqi_values[pollutant],\n",
        "        ax=ax2,\n",
        "        color='red',\n",
        "        label=f'{pollutant} AQI',\n",
        "        linewidth=1.5,\n",
        "        linestyle='--'\n",
        "    )\n",
        "    ax2.set_ylabel(f'{pollutant} AQI', color='red', fontsize=12, fontweight='bold')\n",
        "    ax2.tick_params(axis='y', labelcolor='red')\n",
        "    ax2.legend(loc='upper left', bbox_to_anchor=(0.0, 0.9))\n",
        "\n",
        "    # --- Add AQI category thresholds and shaded regions ---\n",
        "    # Collect legend handles for categories to avoid duplicate entries\n",
        "    category_legend_handles = []\n",
        "\n",
        "    for category, bounds in sorted_categories:\n",
        "        lower = bounds['lower']\n",
        "        upper = bounds['upper']\n",
        "        color = aqi_colors.get(category, 'gray')\n",
        "\n",
        "        # Add shaded region for this AQI category\n",
        "        if upper != np.inf:\n",
        "            ax2.axhspan(lower, upper, color=color, alpha=0.1, zorder=0) # Below lines\n",
        "            # Add horizontal line at upper threshold\n",
        "            line, = ax2.plot([], [], color=color, linestyle=':', linewidth=1.5, alpha=0.7, label=f'{category} AQI')\n",
        "            ax2.axhline(y=upper, color=color, linestyle=':', linewidth=1.5, alpha=0.7, zorder=1)\n",
        "        else:\n",
        "            # For 'Severe' category (open-ended), shade up to the max visible y-limit\n",
        "            ax2.axhspan(lower, ax2.get_ylim()[1], color=color, alpha=0.1, zorder=0)\n",
        "            line, = ax2.plot([], [], color=color, linestyle=':', linewidth=1.5, alpha=0.7, label=f'{category} AQI')\n",
        "            ax2.axhline(y=lower, color=color, linestyle=':', linewidth=1.5, alpha=0.7, zorder=1)\n",
        "        category_legend_handles.append(line)\n",
        "\n",
        "    ax2.set_title(f'{pollutant} Concentration and AQI by Category', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(False) # Disable grid for second axis to avoid clutter\n",
        "\n",
        "    # Combine legends for clarity\n",
        "    h1, l1 = ax1.get_legend_handles_labels()\n",
        "    h2, l2 = ax2.get_legend_handles_labels()\n",
        "    ax2.legend(h1 + h2, l1 + l2, loc='upper left', bbox_to_anchor=(0.0, 1.0))\n",
        "\n",
        "    # Set x-axis labels only for the last subplot (or current if only one)\n",
        "    if i == len(pollutants_to_visualize) - 1:\n",
        "        ax1.set_xlabel('Date and Time', fontsize=12)\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "        for label in ax1.get_xticklabels():\n",
        "            label.set_ha('right')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "plt.show()\n",
        "\n",
        "print(\"Individual pollutant AQI and concentration visualization with categories completed!\")"
      ],
      "metadata": {
        "id": "etHL10g_XIS9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}